{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35 35 35 35 35 35 35 35 35 35]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "fileNames = glob.glob('2019Proj2_train/*.txt')\n",
    "# print(fileNames)\n",
    "\n",
    "\n",
    "def fileReader(file,num_clusters = 50):\n",
    "    '''\n",
    "    This will read the file, and output a discretized value, based on Kmeans clustering\n",
    "    \n",
    "    input:\n",
    "    file: Filename of training data used\n",
    "    num_clusters: number of discrete values based on Kmeans clustering\n",
    "    \n",
    "    output:\n",
    "    kmean.labels: [0,num_clusters) - exclusive of num_clusters number!\n",
    "    '''\n",
    "    a = []\n",
    "    # Creating a list, appending to list, and then making a numpy array\n",
    "    \n",
    "    \n",
    "    with open(file, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            a.append(row)\n",
    "    tmp = np.array(a)\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(tmp)\n",
    "    return kmeans.labels_\n",
    "\n",
    "\n",
    "\n",
    "# This should be trained on all of the data. After doing this I need to make the silos of data\n",
    "\n",
    "num_clusters = 50 # number of discrete values\n",
    "\n",
    "beat_three = fileNames[0:5]; beat3_data = np.array([fileReader(beat3) for beat3 in beat_three])\n",
    "beat_four = fileNames[5:10]; beat4_data = np.array([fileReader(beat4) for beat4 in beat_four])\n",
    "circle = fileNames[10:15]; circle_data = np.array([fileReader(circ) for circ in circle])\n",
    "eight = fileNames[15:20]; eight_data = np.array([fileReader(e) for e in eight])\n",
    "inf = fileNames[20:25]; inf_data = np.array([fileReader(i) for i in inf])\n",
    "wave = fileNames[25:30]; wave_data = np.array([fileReader(wv) for wv in wave])\n",
    "\n",
    "# print(wave_data[:10], \"\\nlast couple: \\n\", wave_data[-10:])\n",
    "\n",
    "print(inf_data[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "print(np.max(inf_data[0]))\n",
    "\n",
    "\n",
    "# Have one array with all the data, and the indexes will tell me where to start/stop\n",
    "\n",
    "\n",
    "# This function was only needed for an older version\n",
    "# def findIndex(*args):\n",
    "#     # input: List of filenames\n",
    "#     i = []\n",
    "#     ind = 0\n",
    "#     for arg in args:\n",
    "#         i.append(ind)\n",
    "#         ind += len(fileReader(arg))\n",
    "        \n",
    "#     return i\n",
    "\n",
    "\n",
    "def dictCreator(names,indexes):\n",
    "    d = {}\n",
    "    rd = {}\n",
    "    for name,index in zip(names,indexes):\n",
    "        d[index] = name\n",
    "        rd[name] = index\n",
    "    return d,rd\n",
    "\n",
    "num_clusters = 50\n",
    "Hidden_states = {'beat_three','beat_four','circle','eight','inf','wave'}\n",
    "observation_states = set(range(num_clusters))\n",
    "\n",
    "# print(val.shape)\n",
    "# print(val[1:10000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior (4,)\n",
      "Emission (4, 5)\n",
      "Transition (4, 4)\n",
      "(6,)\n",
      "(4, 6)\n",
      "alpha:  [[ 0.05       -0.21061758 -0.27532622 -0.29109516 -0.29483303 -0.29567956]\n",
      " [ 0.05       -0.21061758 -0.275309   -0.29080338 -0.2942946  -0.29504598]\n",
      " [ 0.05       -0.21056543 -0.27438126 -0.28908058 -0.29226969 -0.29293723]\n",
      " [ 0.05       -0.21040723 -0.271742   -0.28447367 -0.28703957 -0.28755354]]\n",
      "beta:  [[-0.29460893 -0.29037678 -0.27394143 -0.21218966  0.03668621  1.        ]\n",
      " [-0.29404905 -0.28980381 -0.27194782 -0.20395087  0.03668621  1.        ]\n",
      " [-0.2920351  -0.28804125 -0.27030188 -0.19875994  0.05778625  1.        ]\n",
      " [-0.28684262 -0.28349186 -0.26690442 -0.18785933  0.13423047  1.        ]]\n",
      "optSeq:  [0 0 0 0 3 0]\n",
      "updated P:  [0.25233398 0.25185444 0.25012948 0.2456821 ]\n",
      "tmp:  [[0.36951788 0.         0.         0.        ]\n",
      " [0.3659314  0.37006687 0.         0.        ]\n",
      " [0.36202855 0.3661205  0.51794297 0.        ]\n",
      " [0.         0.35686965 0.5048662  0.83054755]]\n",
      "Updated E:  [1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Finally starting HMM\n",
    "# print(indDict)\n",
    "\n",
    "from scipy.special import logsumexp\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Model():\n",
    "        \n",
    "        def __init__(self, num_HiddenState, num_observationClass):\n",
    "            # Define the initial (A,B, and pi)\n",
    "            self.num_HiddenState = num_HiddenState\n",
    "            self.num_observations = num_observationClass\n",
    "            self.P = self.prior(num_HiddenState)\n",
    "            self.E = self.Emission(num_HiddenState, num_observationClass)\n",
    "            self.T = self.Transition(num_HiddenState)\n",
    "            self.hidden = num_HiddenState\n",
    "            self.observed = num_observationClass\n",
    "            print(\"Prior\",self.P.shape)\n",
    "            print(\"Emission\",self.E.shape)\n",
    "            print(\"Transition\",self.T.shape)\n",
    "    \n",
    "    \n",
    "        def prior(self, num_HiddenState):\n",
    "            p = np.divide(np.ones(num_HiddenState),num_HiddenState)\n",
    "\n",
    "#             p[0] = 1\n",
    "            return p\n",
    "\n",
    "        def Emission(self, num_HiddenState, num_observationClass):\n",
    "            e = np.zeros((num_HiddenState,num_observationClass))\n",
    "            e[:,:] = 1/num_observationClass\n",
    "\n",
    "\n",
    "            return e\n",
    "\n",
    "        def Transition(self, num_HiddenState):\n",
    "            '''\n",
    "            First, we will try creating an ergodic transition matrix. Next, will try a left-right model\n",
    "            '''\n",
    "            t = np.full((num_HiddenState,num_HiddenState),1/num_HiddenState)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            # This will create a left-right matrix. Will try this later, but for now ergodic\n",
    "            \n",
    "#             for i in range(num_HiddenState-2):\n",
    "#                 t[i,i] = 1/3\n",
    "#                 t[i,i+1] = 1/3\n",
    "#                 t[i,i+2] = 1/3\n",
    "\n",
    "#             # Final initializations\n",
    "#             t[num_HiddenState-2,num_HiddenState-2] = 1/2\n",
    "#             t[num_HiddenState-2,num_HiddenState-1] = 1/2\n",
    "\n",
    "#             t[num_HiddenState-1,num_HiddenState-1] = 1\n",
    "\n",
    "\n",
    "            return t\n",
    "        \n",
    "        def forwardInduction(self, sequence, reverse = False):\n",
    "            print(\"too much T\",self.T.shape)\n",
    "            print(\"Emissions\", self.E.shape)\n",
    "            alpha = np.zeros((len(self.P),len(sequence)))\n",
    "            alpha[:,0] = self.P * self.E[:,0]\n",
    "#             print(alpha)\n",
    "\n",
    "            # Now the real fun begins\n",
    "            for i,state in enumerate(sequence,1):\n",
    "                alpha[:,i] = logsumexp(np.multiply(alpha[:,i-1] , self.E[:,i]))\n",
    "            print(alpha)\n",
    "           \n",
    "            \n",
    "            \n",
    "#             # first pass\n",
    "#             alpha[0] = self.P*self.Emission\n",
    "            \n",
    "    \n",
    "            \n",
    "            \n",
    "#             for state in arange(num_states):\n",
    "#                 t_update = logsumexp(t[state] * t[state][]) * log() # Recommended by Anthony for more efficiency\n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Model(4,5)\n",
    "\n",
    "\n",
    "def initialization(Model, sequence):\n",
    "    '''\n",
    "    The forward calculation of the algorithm\n",
    "    '''\n",
    "    print(sequence.shape)\n",
    "    alpha = np.zeros((Model.num_HiddenState,len(sequence)))\n",
    "    alpha[:,0] = np.multiply(Model.P, Model.E[:,0])\n",
    "    print(alpha.shape)\n",
    "    \n",
    "    \n",
    "    for t,time in enumerate(sequence[1:],1):\n",
    "        \n",
    "        for j,state_out in enumerate(range(Model.num_HiddenState)): \n",
    "            alpha[j,t] = logsumexp([np.multiply(alpha[i,t-1],Model.T[j,i]) for i in range(Model.num_HiddenState)]) \\\n",
    "            + math.log(0.2) # I need to make this actually relate to the observations.            \n",
    "    print(\"alpha: \",alpha)\n",
    "    return alpha\n",
    "\n",
    "def initializeBeta(Model,sequence):\n",
    "    '''\n",
    "    The backwards calculation of the forward-backward algorithm\n",
    "    '''\n",
    "    beta = np.zeros((Model.num_HiddenState,len(sequence)))\n",
    "    beta[:,-1] = np.ones((Model.num_HiddenState,))\n",
    "    \n",
    "    for t,_ in enumerate(sequence[:-1],1):\n",
    "        t = len(sequence) - 1 - t\n",
    "        \n",
    "        for j,_ in enumerate(range(Model.num_HiddenState)): \n",
    "            beta[j,t] = logsumexp([np.multiply(beta[i,t+1],Model.T[j,i]) for i in range(Model.num_HiddenState)]) \\\n",
    "            + math.log(0.2) # I need to make this actually relate to the observations.\n",
    "            \n",
    "    print(\"beta: \",beta)\n",
    "    return beta\n",
    "\n",
    "def optimalseq(Model, sequence, alpha, beta):\n",
    "    '''\n",
    "    This will return the optimal state sequence given these observations.\n",
    "    \n",
    "    sequence: refers to observation sequence\n",
    "    \n",
    "    returns:\n",
    "    optSeq: This is the optimal sequence\n",
    "    \n",
    "    '''\n",
    "    gamma = np.zeros((Model.num_HiddenState,len(sequence)))\n",
    "    \n",
    "    for t, _ in enumerate(sequence):\n",
    "        normalization = np.sum(np.multiply(alpha[:,t],beta[:,t]))\n",
    "        tmp = np.multiply(alpha[:,t],beta[:,t])\n",
    "        gamma[:,t] = np.divide(tmp,normalization)\n",
    "        \n",
    "    optSeq = np.argmax(gamma,axis=0)\n",
    "    print(\"optSeq: \", optSeq)\n",
    "    \n",
    "    return optSeq, gamma\n",
    "\n",
    "def EM(Model, sequence, alpha, beta, optimal_sequence,gamma):\n",
    "    '''\n",
    "    The output of xi should be the size of the sequence\n",
    "    '''\n",
    "#     xi = np.zeros((len(optimal_sequence)-1,))\n",
    "    xi = np.zeros((Model.num_HiddenState, Model.num_HiddenState,len(optimal_sequence)-1))\n",
    "#     print(xi.shape)\n",
    "    \n",
    "    for t in range(len(optimal_sequence)-1):\n",
    "        # Hidden states that are inferred for current and one ahead in time\n",
    "        ot = optimal_sequence[t]\n",
    "        ot1 = optimal_sequence[t+1]\n",
    "        \n",
    "        # Observed sequences\n",
    "        st = sequence[t]\n",
    "        st1 = sequence[t+1]\n",
    "\n",
    "\n",
    "        # Very long list comprehension, that forms that array\n",
    "        tmp = np.array([alpha[i,t] * Model.T[i,j] * Model.E[optimal_sequence[t+1],j] * beta[j, optimal_sequence[t+1]] \\\n",
    "        for j in range(Model.num_HiddenState) for i in range(Model.num_HiddenState)])\n",
    "        normalization = np.sum(tmp)\n",
    "\n",
    "        xi[:,:,t] = np.reshape(tmp/normalization,(4,4))\n",
    "        \n",
    "        \n",
    "#         This gamma isn't used. Hope that is ok!\n",
    "    newgamma = np.sum(xi,axis= 0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Update initial probability\n",
    "    print(\"updated P: \",gamma[:,0])\n",
    "    Model.P = gamma[:,0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Update transition probabilities\n",
    "    \n",
    "    Model.T = np.sum(xi,axis = 2) / np.sum(gamma[:,:-1], axis = 1)\n",
    "#         print(\"Updated T: \",np.sum(xi,axis = 2)[i,:] / np.sum(gamma[:,:-1], axis = 0)[i])\n",
    "    print(\"tmp: \",Model.T)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Update emission probabilities\n",
    "    print(\"Updated E: \", np.sum(gamma, axis = 0) / np.sum(gamma, axis = 0))\n",
    "#     Model.E = np.sum(gamma, axis = 0) / np.sum(gamma, axis = 0)\n",
    "\n",
    "    return xi,gamma\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "def baumWelch(Model,sequence):\n",
    "    '''\n",
    "    Describes doing the forward-backward calculation, followed by determining the optimal sequence that is \n",
    "    described by this\n",
    "    \n",
    "    Model: the HMM model that is used, which includes in hidden_states, # different observations\n",
    "    sequence: The sequence that acts as your input\n",
    "    '''\n",
    "    o = set(sequence)\n",
    "    alpha = initialization(Model,sequence)\n",
    "    beta = initializeBeta(Model,sequence)\n",
    "    optSeq, gamma = optimalseq(Model, sequence, alpha, beta)\n",
    "    xi,_ = EM(Model, sequence, alpha,beta,optSeq, gamma)\n",
    "        \n",
    "    \n",
    "baumWelch(model,np.array([5,6,7,8,9,8]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
